{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import absolute_import\n",
      "from __future__ import print_function\n",
      "import tensorflow.python.platform\n",
      "import collections\n",
      "import math\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "from six.moves import urllib\n",
      "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
      "import tensorflow as tf\n",
      "import zipfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 1: Download the data.\n",
      "url = 'http://mattmahoney.net/dc/'\n",
      "def maybe_download(filename, expected_bytes):\n",
      "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
      "  if not os.path.exists(filename):\n",
      "    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
      "  statinfo = os.stat(filename)\n",
      "  if statinfo.st_size == expected_bytes:\n",
      "    print('Found and verified', filename)\n",
      "  else:\n",
      "    print(statinfo.st_size)\n",
      "    raise Exception(\n",
      "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
      "  return filename\n",
      "filename = maybe_download('text8.zip', 31344016)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found and verified text8.zip\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read the data into a string.\n",
      "def read_data(filename):\n",
      "  f = zipfile.ZipFile(filename)\n",
      "  for name in f.namelist():\n",
      "    return f.read(name).split()\n",
      "  f.close()\n",
      "words = read_data(filename)\n",
      "print('Data size', len(words))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Data size 17005207\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
      "vocabulary_size = 50000\n",
      "def build_dataset(words):\n",
      "  count = [['UNK', -1]]\n",
      "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
      "  dictionary = dict()\n",
      "  for word, _ in count:\n",
      "    dictionary[word] = len(dictionary)\n",
      "  data = list()\n",
      "  unk_count = 0\n",
      "  for word in words:\n",
      "    if word in dictionary:\n",
      "      index = dictionary[word]\n",
      "    else:\n",
      "      index = 0  # dictionary['UNK']\n",
      "      unk_count += 1\n",
      "    data.append(index)\n",
      "  count[0][1] = unk_count\n",
      "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
      "  return data, count, dictionary, reverse_dictionary\n",
      "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
      "del words  # Hint to reduce memory.\n",
      "print('Most common words (+UNK)', count[:5])\n",
      "print('Sample data', data[:10])\n",
      "data_index = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
        "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 3: Function to generate a training batch for the skip-gram model.\n",
      "def generate_batch(batch_size, num_skips, skip_window):\n",
      "  global data_index\n",
      "  assert batch_size % num_skips == 0\n",
      "  assert num_skips <= 2 * skip_window\n",
      "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
      "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
      "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
      "  buffer = collections.deque(maxlen=span)\n",
      "  for _ in range(span):\n",
      "    buffer.append(data[data_index])\n",
      "    data_index = (data_index + 1) % len(data)\n",
      "  for i in range(batch_size // num_skips):\n",
      "    target = skip_window  # target label at the center of the buffer\n",
      "    targets_to_avoid = [ skip_window ]\n",
      "    for j in range(num_skips):\n",
      "      while target in targets_to_avoid:\n",
      "        target = random.randint(0, span - 1)\n",
      "      targets_to_avoid.append(target)\n",
      "      batch[i * num_skips + j] = buffer[skip_window]\n",
      "      labels[i * num_skips + j, 0] = buffer[target]\n",
      "    buffer.append(data[data_index])\n",
      "    data_index = (data_index + 1) % len(data)\n",
      "  return batch, labels\n",
      "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
      "for i in range(8):\n",
      "  print(batch[i], '->', labels[i, 0])\n",
      "  print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i, 0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3084 -> 5239\n",
        "originated -> anarchism\n",
        "3084 -> 12\n",
        "originated -> as\n",
        "12 -> 3084\n",
        "as -> originated\n",
        "12 -> 6\n",
        "as -> a\n",
        "6 -> 195\n",
        "a -> term\n",
        "6 -> 12\n",
        "a -> as\n",
        "195 -> 2\n",
        "term -> of\n",
        "195 -> 6\n",
        "term -> a\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 4: Build and train a skip-gram model.\n",
      "batch_size = 128\n",
      "embedding_size = 128  # Dimension of the embedding vector.\n",
      "skip_window = 1       # How many words to consider left and right.\n",
      "num_skips = 2         # How many times to reuse an input to generate a label.\n",
      "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
      "# validation samples to the words that have a low numeric ID, which by\n",
      "# construction are also the most frequent.\n",
      "valid_size = 16     # Random set of words to evaluate similarity on.\n",
      "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
      "valid_examples = np.array(random.sample(np.arange(valid_window), valid_size))\n",
      "num_sampled = 64    # Number of negative examples to sample.\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "  # Input data.\n",
      "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
      "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
      "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
      "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
      "  with tf.device('/cpu:0'):\n",
      "    # Look up embeddings for inputs.\n",
      "    embeddings = tf.Variable(\n",
      "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
      "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
      "    # Construct the variables for the NCE loss\n",
      "    nce_weights = tf.Variable(\n",
      "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
      "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
      "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
      "  # Compute the average NCE loss for the batch.\n",
      "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
      "  # time we evaluate the loss.\n",
      "  loss = tf.reduce_mean(\n",
      "      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
      "                     num_sampled, vocabulary_size))\n",
      "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
      "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
      "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
      "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
      "  normalized_embeddings = embeddings / norm\n",
      "  valid_embeddings = tf.nn.embedding_lookup(\n",
      "      normalized_embeddings, valid_dataset)\n",
      "  similarity = tf.matmul(\n",
      "      valid_embeddings, normalized_embeddings, transpose_b=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 5: Begin training.\n",
      "#num_steps = 100001\n",
      "num_steps = 100001\n",
      "with tf.Session(graph=graph) as session:\n",
      "  # We must initialize all variables before we use them.\n",
      "  tf.initialize_all_variables().run()\n",
      "  print(\"Initialized\")\n",
      "  average_loss = 0\n",
      "  for step in xrange(num_steps):\n",
      "    batch_inputs, batch_labels = generate_batch(\n",
      "        batch_size, num_skips, skip_window)\n",
      "    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
      "    # We perform one update step by evaluating the optimizer op (including it\n",
      "    # in the list of returned values for session.run()\n",
      "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
      "    average_loss += loss_val\n",
      "    if step % 2000 == 0:\n",
      "      if step > 0:\n",
      "        average_loss /= 2000\n",
      "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
      "      print(\"Average loss at step \", step, \": \", average_loss)\n",
      "      average_loss = 0\n",
      "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
      "    if step % 10000 == 0:\n",
      "      sim = similarity.eval()\n",
      "      for i in xrange(valid_size):\n",
      "        valid_word = reverse_dictionary[valid_examples[i]]\n",
      "        top_k = 8 # number of nearest neighbors\n",
      "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
      "        log_str = \"Nearest to %s:\" % valid_word\n",
      "        for k in xrange(top_k):\n",
      "          close_word = reverse_dictionary[nearest[k]]\n",
      "          log_str = \"%s %s,\" % (log_str, close_word)\n",
      "        print(log_str)\n",
      "  final_embeddings = normalized_embeddings.eval()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 :  113.66667265\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000 :  52.21462291\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000 :  33.4993881741\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000 :  23.9318036985\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10000 :  17.7235456834\n",
        "Nearest to two: one, nine, three, reginae, four, gland, victoriae, dreyfus,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, as, and, for, baltic, genuine, in,\n",
        "Nearest to up: gollancz, tubing, stable, exams, three, nautical, clark, publishing,\n",
        "Nearest to than: developers, spotted, changing, attacks, judgments, mathbf, awards, actions,\n",
        "Nearest to six: reginae, nine, zero, mathbf, victoriae, gland, implicit, seven,\n",
        "Nearest to see: popularity, schiff, fans, rugby, waite, weaker, we, justin,\n",
        "Nearest to s: and, the, ha, chile, reginae, of, album, zero,\n",
        "Nearest to most: corporation, led, mourned, means, clashed, afghani, isi, gloves,\n",
        "Nearest to b: forall, sporting, dreyfus, linear, associated, victoriae, philosopher, affiliated,\n",
        "Nearest to zero: nine, reginae, victoriae, eight, potato, gland, six, one,\n",
        "Nearest to first: and, extraction, studded, company, layout, tended, reginae, victoriae,\n",
        "Nearest to a: the, metis, gollancz, and, this, or, both, louise,\n",
        "Nearest to or: and, acquired, roland, a, perhaps, afghani, victoriae, of,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: art, metis, the, rotors, aq, infertile, schemata, clyde,\n",
        "Nearest to time: saints, weeks, facility, mirrors, cardinal, seemed, vs, jungle,\n",
        "Nearest to been: reginae, advance, aq, alkali, finalist, described, condemned, analogue,\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Average loss at step  0 :  284.391876221\n",
        "Nearest to two: asl, crucified, suharto, stations, donny, blyth, jumo, snows,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: sinned, her, jurisdictions, lud, shrank, esaf, chanter, bee,\n",
        "Nearest to up: crackdown, grad, threw, fabulous, gibbons, captives, epicenter, val,\n",
        "Nearest to than: pirin, judaica, pfc, feud, belvedere, chongqing, ahaziah, pulses,\n",
        "Nearest to six: edif, snowman, schirra, mintz, medicinal, merkel, gaku, undecidable,\n",
        "Nearest to see: alec, mysteriously, mommsen, profoundly, mergers, stillness, dickinson, nn,\n",
        "Nearest to s: devas, chifley, informational, simmons, owsley, electrolytes, circumscribed, everlasting,\n",
        "Nearest to most: mahal, dominoes, sodomy, thiele, mpv, hamas, arcueil, sadr,\n",
        "Nearest to b: knows, ucl, interfered, magda, italic, theocratic, minerva, townsfolk,\n",
        "Nearest to zero: loewe, broadleaf, pflp, veyron, window, vilhelm, archaeologist, robe,\n",
        "Nearest to first: smoke, manner, mislead, belated, jacobi, inactivity, charging, appropriation,\n",
        "Nearest to a: flavored, advert, litters, boleslav, consolidated, nevertheless, wulf, coru,\n",
        "Nearest to or: harbors, mouth, arbitration, evolved, resistance, marcellus, bahamian, clearer,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: minamoto, mariam, competences, conceptualized, appellant, pronounces, grimoires, decidable,\n",
        "Nearest to time: knoll, instantly, entailed, inclusive, have, unmanned, cronquist, le,\n",
        "Nearest to been: halfway, tamerlane, ashley, mnt, puck, vauxhall, elementals, anacreon,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 :  113.747537514\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000 :  53.2729524565\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000 :  33.38683304\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000 :  23.4528169975\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10000 :  17.21954197\n",
        "Nearest to two: one, ramps, three, zero, dasyprocta, eight, stations, four,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, has, nine, in, on, bataan, aquila,\n",
        "Nearest to up: equally, its, threw, alternatives, tank, reduced, keyboards, val,\n",
        "Nearest to than: pac, statements, statement, wildlife, consonantal, furry, receive, regency,\n",
        "Nearest to six: nine, zero, dasyprocta, five, agouti, seven, yang, eight,\n",
        "Nearest to see: agouti, third, alec, lunar, perry, zero, mergers, nn,\n",
        "Nearest to s: zero, and, aquila, oxidised, sleep, abelian, success, offerings,\n",
        "Nearest to most: all, natural, combine, cth, impossible, storage, ascii, carrier,\n",
        "Nearest to b: knows, aquila, one, sahih, d, niches, study, facto,\n",
        "Nearest to zero: nine, three, six, five, four, seven, aquila, rounds,\n",
        "Nearest to first: of, follows, pushed, manner, ramps, aquila, numerical, west,\n",
        "Nearest to a: the, aquila, UNK, this, and, trough, dasyprocta, lisbon,\n",
        "Nearest to or: and, in, resistance, mouth, zero, phenotype, contrast, an,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: vols, the, a, sales, refuge, reading, timeline, preprocessor,\n",
        "Nearest to time: le, inclusive, instantly, absalom, dasyprocta, discussion, wildlife, solzhenitsyn,\n",
        "Nearest to been: anacreon, zero, halfway, absalom, shall, mathematica, reactions, springfield,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12000 :  13.9891076399\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14000 :  11.5692665926\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16000 :  10.0655569224\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18000 :  8.53690909505\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000 :  8.14929216611\n",
        "Nearest to two: three, four, one, five, eight, zero, nine, six,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, has, were, as, if, aquila, bataan,\n",
        "Nearest to up: equally, its, alternatives, threw, val, card, keyboards, abkhazia,\n",
        "Nearest to than: statements, pac, feud, anselm, statement, rowan, draconis, fracture,\n",
        "Nearest to six: nine, eight, five, zero, seven, four, three, two,\n",
        "Nearest to see: alec, third, agouti, akita, zero, perry, eucalyptus, nn,\n",
        "Nearest to s: and, zero, aquila, of, the, or, oxidised, two,\n",
        "Nearest to most: hamas, all, mahal, natural, storage, akh, combine, ladders,\n",
        "Nearest to b: d, knows, akita, aquila, sahih, and, niches, study,\n",
        "Nearest to zero: nine, eight, five, seven, four, six, three, akita,\n",
        "Nearest to first: akita, of, follows, pushed, ramps, lawgiver, aquila, manner,\n",
        "Nearest to a: the, this, akita, aquila, appomattox, aba, their, akh,\n",
        "Nearest to or: and, in, akita, s, abet, phenotype, nine, three,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: the, vols, abet, systematic, akita, timeline, a, refuge,\n",
        "Nearest to time: le, instantly, inclusive, akh, absalom, dasyprocta, cth, discussion,\n",
        "Nearest to been: anacreon, by, ladders, powerbook, was, noah, cpa, were,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 22000 :  7.16874054289\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24000 :  7.14824963605\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26000 :  6.63803252101\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 28000 :  5.93898261344\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 30000 :  5.97389832103\n",
        "Nearest to two: three, four, one, five, seven, six, eight, zero,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, has, were, by, as, microseconds, be,\n",
        "Nearest to up: its, equally, crackdown, card, val, alternatives, periods, threw,\n",
        "Nearest to than: or, pirin, anselm, pac, statements, eight, feud, four,\n",
        "Nearest to six: eight, four, five, nine, seven, three, zero, two,\n",
        "Nearest to see: alec, agouti, third, akita, perry, rounded, recitative, bahn,\n",
        "Nearest to s: and, aquila, zero, his, hind, or, two, agouti,\n",
        "Nearest to most: hamas, all, mahal, natural, combine, storage, akh, client,\n",
        "Nearest to b: d, UNK, knows, theocratic, ucl, aquila, akita, niches,\n",
        "Nearest to zero: five, eight, seven, nine, six, four, three, two,\n",
        "Nearest to first: akita, second, pushed, follows, lawgiver, aquila, ramps, smoke,\n",
        "Nearest to a: the, akita, aquila, appomattox, this, his, eight, recitative,\n",
        "Nearest to or: and, recitative, eight, akita, three, five, abet, zero,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: the, vols, cameras, johansen, timeline, yauch, systematic, iranians,\n",
        "Nearest to time: lemmy, le, instantly, inclusive, akh, inflation, cth, dasyprocta,\n",
        "Nearest to been: anacreon, was, were, by, halfway, ladders, be, powerbook,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 32000 :  5.80773169518\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 34000 :  5.74555670881\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 36000 :  5.69826484537\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 38000 :  5.29722458315\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 40000 :  5.4192066437\n",
        "Nearest to two: three, four, one, five, six, thibetanus, seven, eight,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, has, were, be, eight, microseconds, nine,\n",
        "Nearest to up: equally, its, crackdown, card, anabolic, out, them, brahmaputra,\n",
        "Nearest to than: or, pirin, pac, anselm, statements, paulinus, hoover, ahaziah,\n",
        "Nearest to six: eight, four, five, seven, three, nine, two, one,\n",
        "Nearest to see: perfective, alec, agouti, akita, bahn, rounded, perry, third,\n",
        "Nearest to s: his, and, hind, zero, aquila, of, roshan, eight,\n",
        "Nearest to most: hamas, all, mahal, journey, combine, storage, justinian, natural,\n",
        "Nearest to b: d, knows, theocratic, UNK, aquila, akita, niches, sahih,\n",
        "Nearest to zero: eight, seven, five, four, six, nine, three, two,\n",
        "Nearest to first: second, akita, lawgiver, apologia, pushed, follows, during, black,\n",
        "Nearest to a: the, thibetanus, aquila, appomattox, this, five, any, their,\n",
        "Nearest to or: and, eight, recitative, five, four, akita, three, six,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: systematic, sammon, cameras, the, yauch, johansen, vols, asa,\n",
        "Nearest to time: inclusive, lemmy, instantly, entailed, akh, dasyprocta, le, wildlife,\n",
        "Nearest to been: was, be, by, were, anacreon, halfway, ladders, had,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 42000 :  5.43811114037\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 44000 :  5.36228540671\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46000 :  5.29351834667\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 48000 :  5.29977854049\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50000 :  5.1184409337\n",
        "Nearest to two: three, four, five, six, one, eight, seven, thibetanus,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, has, marmoset, microcebus, microsite, bz, callithrix,\n",
        "Nearest to up: equally, out, them, crackdown, card, anabolic, its, brahmaputra,\n",
        "Nearest to than: or, paulinus, anselm, pirin, entrenchment, pac, hoover, statements,\n",
        "Nearest to six: eight, five, four, seven, three, nine, two, zero,\n",
        "Nearest to see: perfective, holographic, but, alec, agouti, akita, bahn, rounded,\n",
        "Nearest to s: and, his, hind, zero, aquila, tamarin, capuchin, cebus,\n",
        "Nearest to most: all, hamas, journey, storage, mahal, justinian, combine, wang,\n",
        "Nearest to b: d, theocratic, UNK, knows, akita, niches, aquila, j,\n",
        "Nearest to zero: five, eight, six, seven, four, nine, three, tamarin,\n",
        "Nearest to first: second, michelob, akita, lawgiver, during, relied, apologia, last,\n",
        "Nearest to a: the, michelob, microcebus, any, thibetanus, aquila, this, tamarin,\n",
        "Nearest to or: and, microsite, microcebus, callithrix, recitative, akita, tamarin, five,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: microcebus, systematic, yauch, sammon, the, cameras, johansen, iranians,\n",
        "Nearest to time: inclusive, entailed, lemmy, knoll, instantly, akh, sweeteners, wildlife,\n",
        "Nearest to been: was, be, were, by, anacreon, riviera, halfway, had,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52000 :  4.94872273672\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 54000 :  4.9503735112\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 56000 :  5.17486382568\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 58000 :  5.08397675788\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 60000 :  4.84712771845\n",
        "Nearest to two: three, four, six, one, five, thibetanus, seven, eight,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, has, are, microcebus, be, were, perfective, marmoset,\n",
        "Nearest to up: out, them, thaler, equally, crackdown, card, anabolic, brahmaputra,\n",
        "Nearest to than: or, and, paulinus, anselm, pirin, marmoset, hoover, entrenchment,\n",
        "Nearest to six: eight, four, seven, five, three, nine, zero, two,\n",
        "Nearest to see: perfective, holographic, alec, but, rounded, agouti, akita, perry,\n",
        "Nearest to s: dinar, hind, aquila, and, capuchin, or, his, zero,\n",
        "Nearest to most: all, hamas, some, more, wang, restlessness, justinian, journey,\n",
        "Nearest to b: d, UNK, theocratic, j, knows, niches, akita, aquila,\n",
        "Nearest to zero: eight, five, six, seven, nine, four, dinar, tamarin,\n",
        "Nearest to first: mitral, second, michelob, akita, last, lawgiver, during, clo,\n",
        "Nearest to a: the, michelob, microcebus, dinar, thibetanus, cebus, any, tamarin,\n",
        "Nearest to or: and, microsite, microcebus, callithrix, akita, dinar, tamarin, thibetanus,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: microcebus, the, systematic, sammon, yauch, cameras, iranians, johansen,\n",
        "Nearest to time: entailed, knoll, inclusive, way, lemmy, wildlife, cronquist, sweeteners,\n",
        "Nearest to been: be, was, were, by, had, anacreon, become, riviera,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 62000 :  4.89094043171\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 64000 :  4.8990746448\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66000 :  4.98241273189\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 68000 :  4.92899131668\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 70000 :  4.8638509804\n",
        "Nearest to two: three, four, six, five, one, seven, thibetanus, eight,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, has, are, microcebus, marmoset, perfective, were, microseconds,\n",
        "Nearest to up: out, them, equally, cet, crackdown, thaler, anabolic, card,\n",
        "Nearest to than: or, anselm, paulinus, tabby, and, marmoset, like, atal,\n",
        "Nearest to six: five, four, seven, eight, three, nine, two, zero,\n",
        "Nearest to see: perfective, holographic, but, alec, agouti, akita, bahn, perry,\n",
        "Nearest to s: hind, dinar, his, aquila, elly, recognised, zero, capuchin,\n",
        "Nearest to most: all, hamas, some, more, many, restlessness, journey, wang,\n",
        "Nearest to b: d, theocratic, UNK, gico, niches, knows, j, sahih,\n",
        "Nearest to zero: five, eight, six, seven, four, nine, dinar, tamarin,\n",
        "Nearest to first: second, mitral, last, michelob, kyushu, akita, politecnico, during,\n",
        "Nearest to a: the, michelob, microcebus, any, aquila, dinar, thibetanus, cebus,\n",
        "Nearest to or: and, microsite, microcebus, callithrix, dinar, thibetanus, akita, tamarin,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: systematic, yauch, sammon, microcebus, cameras, iranians, remakes, johansen,\n",
        "Nearest to time: knoll, way, entailed, sweeteners, inclusive, cegep, lemmy, geographically,\n",
        "Nearest to been: be, was, were, become, had, by, anacreon, halfway,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 72000 :  4.91659459496\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 74000 :  4.87023398864\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 76000 :  4.82443339002\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 78000 :  4.74382984579\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 80000 :  4.82398405135\n",
        "Nearest to two: three, five, four, six, seven, one, eight, thibetanus,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, has, are, microcebus, be, marmoset, microseconds, perfective,\n",
        "Nearest to up: out, them, cet, thaler, crackdown, equally, him, brahmaputra,\n",
        "Nearest to than: or, anselm, paulinus, like, tabby, marmoset, chongqing, hoover,\n",
        "Nearest to six: eight, seven, four, five, three, nine, two, zero,\n",
        "Nearest to see: perfective, holographic, but, alec, akita, agouti, external, perry,\n",
        "Nearest to s: his, dinar, hind, aquila, and, recognised, tamarin, niagara,\n",
        "Nearest to most: some, more, all, hamas, many, restlessness, hens, wang,\n",
        "Nearest to b: d, theocratic, UNK, six, j, gico, knows, three,\n",
        "Nearest to zero: five, seven, eight, six, nine, four, dinar, tamarin,\n",
        "Nearest to first: second, mitral, last, michelob, kyushu, akita, politecnico, lawgiver,\n",
        "Nearest to a: the, any, microcebus, michelob, aquila, dinar, thibetanus, cebus,\n",
        "Nearest to or: and, microsite, microcebus, callithrix, dinar, thibetanus, akita, but,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: systematic, sammon, microcebus, cameras, yauch, iranians, the, remakes,\n",
        "Nearest to time: way, cegep, knoll, lemmy, sweeteners, inclusive, wildlife, entailed,\n",
        "Nearest to been: be, was, were, become, had, by, anacreon, riviera,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 82000 :  4.79285565424\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 84000 :  4.68671597224\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 86000 :  4.80518293417\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 88000 :  4.66295897394\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 90000 :  4.74261130917\n",
        "Nearest to two: three, four, five, six, one, seven, eight, thibetanus,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, has, are, microcebus, vert, callithrix, marmoset, microsite,\n",
        "Nearest to up: out, them, him, cet, thaler, card, crackdown, anabolic,\n",
        "Nearest to than: or, and, anselm, like, marmoset, tabby, paulinus, atal,\n",
        "Nearest to six: seven, four, eight, five, nine, three, two, zero,\n",
        "Nearest to see: perfective, holographic, but, alec, akita, external, agouti, perry,\n",
        "Nearest to s: his, hind, dinar, aquila, and, was, the, cebus,\n",
        "Nearest to most: more, some, all, many, hamas, restlessness, loudspeakers, tuna,\n",
        "Nearest to b: d, theocratic, sahih, j, gico, niches, knows, ucl,\n",
        "Nearest to zero: five, eight, four, seven, six, nine, dinar, tamarin,\n",
        "Nearest to first: second, mitral, last, michelob, kyushu, akita, during, bp,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to a: the, any, michelob, microcebus, thibetanus, aquila, dinar, tamarin,\n",
        "Nearest to or: and, microsite, than, microcebus, dinar, callithrix, akita, cebus,\n",
        "Nearest to an: systematic, sammon, microcebus, yauch, cameras, iranians, the, johansen,\n",
        "Nearest to time: knoll, cegep, way, entailed, lemmy, sweeteners, inclusive, wildlife,\n",
        "Nearest to been: be, was, become, were, by, had, anacreon, riviera,\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 92000 :  4.73434540188\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 94000 :  4.66004732239\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 96000 :  4.69497040427\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 98000 :  4.63394944358\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100000 :  4.52745520508\n",
        "Nearest to two: three, four, one, five, six, thibetanus, seven, dinar,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to is: was, are, has, microcebus, were, marmoset, although, vert,\n",
        "Nearest to up: out, them, him, cet, thaler, crackdown, card, characterize,\n",
        "Nearest to than: or, jmp, marmoset, anselm, like, tabby, but, atal,\n",
        "Nearest to six: seven, four, five, eight, three, nine, zero, tamarin,\n",
        "Nearest to see: perfective, holographic, devo, akita, but, agouti, external, replicants,\n",
        "Nearest to s: dinar, hind, his, aquila, tamarin, iit, agouti, superfamily,\n",
        "Nearest to most: more, many, all, some, hamas, restlessness, loudspeakers, gunther,\n",
        "Nearest to b: d, theocratic, UNK, sahih, gico, j, niches, akita,\n",
        "Nearest to zero: eight, five, six, four, seven, dinar, tamarin, nine,\n",
        "Nearest to first: second, last, mitral, michelob, netbios, politecnico, during, akita,\n",
        "Nearest to a: the, any, michelob, microcebus, aquila, dinar, thibetanus, netbios,\n",
        "Nearest to or: and, microsite, dinar, than, microcebus, but, callithrix, tamarin,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nearest to an: sammon, systematic, yauch, microcebus, cameras, iranians, selwyn, johansen,\n",
        "Nearest to time: way, cegep, knoll, entailed, lemmy, dasyprocta, inclusive, metaphor,\n",
        "Nearest to been: be, become, was, were, had, by, anacreon, brim,\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 6: Visualize the embeddings.\n",
      "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
      "  assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
      "  plt.figure(figsize=(18, 18))  #in inches\n",
      "  for i, label in enumerate(labels):\n",
      "    x, y = low_dim_embs[i,:]\n",
      "    plt.scatter(x, y)\n",
      "    plt.annotate(label,\n",
      "                 xy=(x, y),\n",
      "                 xytext=(5, 2),\n",
      "                 textcoords='offset points',\n",
      "                 ha='right',\n",
      "                 va='bottom')\n",
      "  plt.savefig(filename)\n",
      "try:\n",
      "  from sklearn.manifold import TSNE\n",
      "  import matplotlib.pyplot as plt\n",
      "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
      "  plot_only = 500\n",
      "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
      "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
      "  plot_with_labels(low_dim_embs, labels)\n",
      "except ImportError:\n",
      "  print(\"Please install sklearn and matplotlib to visualize embeddings.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Please install sklearn and matplotlib to visualize embeddings.\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}